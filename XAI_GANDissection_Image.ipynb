{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "XAI_GANDissection_Image.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gFfXOniDtiBJ",
        "0Xv6lQC4tiBQ",
        "acEFlbTGtiBS",
        "Q2e9hPZCtiBT",
        "su4jS76UtiBW",
        "xHX3fN1UtiBZ",
        "k1fIczqFtiBb"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flecue/xai-aaai2021/blob/main/XAI_GANDissection_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIzJwmpmtiA6"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "pip install ninja 2>> install.log\n",
        "git clone https://github.com/SIDN-IAP/global-model-repr.git tutorial_code 2>> install.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e686xOZEtiA_"
      },
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/tutorial_code')\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")  \n",
        "except:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNCB9nOGtiA_"
      },
      "source": [
        "# GAN Dissection\n",
        "\n",
        "Our mission: look inside a GAN generator to see what it does.\n",
        "\n",
        "We begin with some imports and jupyter setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEPbq6bvtiBB"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kM8ZL5ltiBB"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running pytorch', torch.__version__, 'using', device.type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfoFKIg0tiBC"
      },
      "source": [
        "## Instantiating a pretrained GAN generator.\n",
        "\n",
        "We're going to use a progressive GAN.\n",
        "\n",
        "Below I download and instantiate a model for outdoor churches.\n",
        "\n",
        "You can uncomment the model of your choice.\n",
        "\n",
        "After we create the model, I just print out all the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "u7EcbqmVtiBD"
      },
      "source": [
        "import torchvision\n",
        "import torch.hub\n",
        "from netdissect import nethook, proggan\n",
        "\n",
        "# n = 'proggan_bedroom-d8a89ff1.pth'\n",
        "n = 'proggan_churchoutdoor-7e701dd5.pth'\n",
        "# n = 'proggan_conferenceroom-21e85882.pth'\n",
        "# n = 'proggan_diningroom-3aa0ab80.pth'\n",
        "# n = 'proggan_kitchen-67f1e16c.pth'\n",
        "# n = 'proggan_livingroom-5ef336dd.pth'\n",
        "# n = 'proggan_restaurant-b8578299.pth'\n",
        "\n",
        "url = 'http://gandissect.csail.mit.edu/models/' + n\n",
        "try:\n",
        "    sd = torch.hub.load_state_dict_from_url(url) # pytorch 1.1\n",
        "except:\n",
        "    sd = torch.hub.model_zoo.load_url(url) # pytorch 1.0\n",
        "model = proggan.from_state_dict(sd).to(device)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp2H5X08tiBD"
      },
      "source": [
        "# Exercise 1 - Directly inspect the internal representation\n",
        "\n",
        "In this exercise we just run the code provided and scrutinize the activations directly.\n",
        "\n",
        "## Running the model.\n",
        "\n",
        "The GAN generator is just a function z->x that transforms random z to realistic images x.\n",
        "\n",
        "To generate images, all we need is a source of random z.  Let's make a micro dataset with a few random z."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpP3dgJYtiBE"
      },
      "source": [
        "from netdissect import zdataset\n",
        "\n",
        "SAMPLE_SIZE = 50 # Increase this for better results (but slower to run)\n",
        "zds = zdataset.z_dataset_for_model(model, size=SAMPLE_SIZE, seed=5555)\n",
        "len(zds), zds[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjV06ugTtiBE"
      },
      "source": [
        "We can just invoke model(z[None,...]) to generate a single image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Ji0vJH3etiBF"
      },
      "source": [
        "# Look at the output data - print a few pixel values\n",
        "model(zds[0][0][None,...].to(device))[0,0,:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn-zEBDbtiBG"
      },
      "source": [
        "## Visualizing the output.\n",
        "\n",
        "The netdissect toolkit comes with a few simple visualization tools for examining images in notebooks.\n",
        "\n",
        "  * renormalize turns tensors that were normalized as [-1...1] back into PIL images.\n",
        "  * show takes nested arrays of images and text and lays then out as grids and tables.\n",
        "  \n",
        "Let's look at the images we created with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GBe-kystiBH"
      },
      "source": [
        "from netdissect import renormalize, show\n",
        "# from IPython.display import display\n",
        "\n",
        "show([\n",
        "    [renormalize.as_image(model(z[None,...].to(device))[0])]\n",
        "    for [z] in zds\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRHFhlltiBH"
      },
      "source": [
        "## Hooking a model with InstrumentedModel\n",
        "\n",
        "To analyze what a model is doing inside, we can wrap it with an InstrumentedModel, which makes it easy to hook or modify a particular layer.\n",
        "\n",
        "InstrumentedModel adds a few useful functions for inspecting a model, including:\n",
        "   * `model.retain_layer('layername')` - hooks a layer to hold on to its output after computation\n",
        "   * `model.retained_layer('layername')` - returns the retained data from the last computation\n",
        "   * `model.edit_layer('layername', rule=...)` - runs the `rule` function after the given layer\n",
        "   * `model.remove_edits()` - removes editing rules\n",
        "\n",
        "Let's setup `retain_layer` now.  We'll pick a layer sort of in the early-middle of the generator.  You can pick whatever you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiZ33ADAtiBH"
      },
      "source": [
        "# TODO: add a summary of what InstrumentedModel can do.\n",
        "# retain a layer, get a retined layer, edit a layer\n",
        "\n",
        "from netdissect import nethook\n",
        "\n",
        "# Don't re-wrap it, if it's already wrapped (e.g., if you press enter twice)\n",
        "if not isinstance(model, nethook.InstrumentedModel):\n",
        "    model = nethook.InstrumentedModel(model)\n",
        "model.retain_layer('layer4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESrRojLitiBI"
      },
      "source": [
        "Now we can run the model and inspect the internal units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5LyaKZTtiBJ"
      },
      "source": [
        "# Run the model\n",
        "img = model(zds[0][0][None,...].to(device))\n",
        "\n",
        "# As a side-effect, the model has retained the output of layer4.\n",
        "acts = model.retained_layer('layer4')\n",
        "\n",
        "# We can look at it.  How much data is it?\n",
        "acts.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dByb_xtiBJ"
      },
      "source": [
        "# Let's just look at the 0th convolutional channel.\n",
        "print(acts[0,0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFfXOniDtiBJ"
      },
      "source": [
        "## Visualizing activation data\n",
        "\n",
        "It can be informative to visualize activation data instead of just looking at the numbers.\n",
        "\n",
        "Net dissection comes with an ImageVisualizer object for visualizing grid data as an image in a few different ways.  Here is a heatmap of the array above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgo94l4jtiBJ"
      },
      "source": [
        "from netdissect import imgviz\n",
        "iv = imgviz.ImageVisualizer(100)\n",
        "iv.heatmap(acts[0,1], mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bpy3ZiAtiBK"
      },
      "source": [
        "If you tell IV to dereference the activations for you, it scales heatmaps according to global statistics.\n",
        "\n",
        "What is happening with unit 418?\n",
        "\n",
        "Each unit has a different scale, which makes the heatmaps harder to interpret.\n",
        "\n",
        "We can normalize the scales by collecting stats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSCSTyaytiBQ"
      },
      "source": [
        "show(\n",
        "    [['unit %d' % u,\n",
        "      [iv.image(img[0])],\n",
        "      [iv.masked_image(img[0], acts, (0,u))],\n",
        "      [iv.heatmap(acts, (0,u), mode='nearest')],\n",
        "     ] for u in range(414, 420)]  \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xv6lQC4tiBQ"
      },
      "source": [
        "## Collecting quantile statistics for every unit\n",
        "\n",
        "We want to know per-channel minimum or maximum values, means, medians, quantiles, etc.\n",
        "\n",
        "We want to treat each pixel as its own sample for all the channels.  For example, here are the activations for one image as an 8x8 tensor over with 512 channels.  We can disregard the geometry and just look at it as a 64x512 sample matrix, that is 64 samples of 512-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JdLlcQltiBQ"
      },
      "source": [
        "print(acts.shape)\n",
        "print(acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1]).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6esJ8kPtiBR"
      },
      "source": [
        "Net dissection has a tally package that tracks quantiles over large samples.\n",
        "\n",
        "To use it, just define a function that returns sample matrices like the 64x512 above, and then it will call your function on every batch and tally up the statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkQTPL4FtiBR"
      },
      "source": [
        "from netdissect import tally\n",
        "\n",
        "# To collect stats, define a function that returns 2d [samples, units]\n",
        "def compute_samples(zbatch):\n",
        "    _ = model(zbatch.to(device))          # run the model\n",
        "    acts = model.retained_layer('layer4') # get the activations, and flatten\n",
        "    return acts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
        "\n",
        "# Then tally_quantile will run your function over the whole dataset to collect quantile stats\n",
        "rq = tally.tally_quantile(compute_samples, zds)\n",
        "\n",
        "# Print out the median value for the first 20 channels\n",
        "rq.quantiles(0.5)[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvCnR2LptiBR"
      },
      "source": [
        "torch.tensor(3).dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acEFlbTGtiBS"
      },
      "source": [
        "## Exploring quantiles\n",
        "\n",
        "The rq object tracks a sketch of all the quantiles of the sampled data.  For example, what is the mean, median, and percentile value for each unit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010XcPzttiBS"
      },
      "source": [
        "# This tells me now, for example, what the means are for channel,\n",
        "# rq.mean()\n",
        "# what median is,\n",
        "# rq.quantiles(0.5)\n",
        "# Or what the 99th percentile quantile is.\n",
        "# rq.quantiles(0.99)\n",
        "\n",
        "(rq.quantiles(0.8) > 0).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJhMnuNZtiBS"
      },
      "source": [
        "The quantiles can be plugged directly into the ImageVisualizer to put heatmaps on an informative per-unit scale.  When you do this:\n",
        "\n",
        "   * Heatmaps are shown on a scale from black to white from 1% lowest to the 99% highest value.\n",
        "   * Masked image lassos are shown at a 95% percentile level (by default, can be changed).\n",
        "   \n",
        "Now unit 418 doesn't drown out the other ones in the visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5Wg1Kw6vtiBS"
      },
      "source": [
        "iv = imgviz.ImageVisualizer(100, quantiles=rq)\n",
        "show([\n",
        "    [  # for every unit, make a block containing\n",
        "       'unit %d' % u,         # the unit number\n",
        "       [iv.image(img[0])],    # the unmodified image\n",
        "       [iv.masked_image(img[0], acts, (0,u))], # the masked image\n",
        "       [iv.heatmap(acts, (0,u), mode='nearest')], # the heatmap\n",
        "    ]\n",
        "    for u in range(414, 420)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2e9hPZCtiBT"
      },
      "source": [
        "## Visualizing top-activating images\n",
        "\n",
        "A useful way to visualize units is to sort a sample, in order of highest activation.  tally_topk does this.\n",
        "\n",
        "Like torch.topk, it returns both the top k values and the top k indexes.  But instead of acting on a single tensor, it iterates over the whole data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egd30dk0tiBT"
      },
      "source": [
        "def compute_image_max(zbatch):\n",
        "    image_batch = model(zbatch.to(device))\n",
        "    return model.retained_layer('layer4').max(3)[0].max(2)[0]\n",
        "\n",
        "topk = tally.tally_topk(compute_image_max, zds)\n",
        "topk.result()[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWNDFsUItiBU"
      },
      "source": [
        "# For each unit, this function prints out unit masks from the top-activating images\n",
        "def unit_viz_row(unitnum, percent_level=0.95):\n",
        "    out = []\n",
        "    for imgnum in topk.result()[1][unitnum][:8]:\n",
        "        img = model(zds[imgnum][0][None,...].to(device))\n",
        "        acts = model.retained_layer('layer4')\n",
        "        out.append([imgnum.item(),\n",
        "                    [iv.masked_image(img[0], acts, (0, unitnum), percent_level=percent_level)],\n",
        "                   ])\n",
        "    return out\n",
        "\n",
        "show(unit_viz_row(365))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_rmQjRtiBU"
      },
      "source": [
        "# Exercise 2 - Evaluate matches with semantic concepts\n",
        "\n",
        "Do the filters match any semantic concepts?  To systematically examine this question,\n",
        "we have pretrained (using lots of labeled data) a semantic segmentation network to recognize\n",
        "a few hundred classes of objects, parts, and textures.\n",
        "\n",
        "Run the code in this section to look for matches between filters in our GAN and semantic\n",
        "segmentation clases.\n",
        "\n",
        "## Labeling semantics within the generated images\n",
        "\n",
        "Let's quantify what's inside these images by segmenting them.\n",
        "\n",
        "First, we create a segmenter network.  (We use the Unified Perceptual Parsing segmenter by Xiao, et al. (https://arxiv.org/abs/1807.10221).\n",
        "\n",
        "Note that the segmenter we use here requires a GPU.\n",
        "\n",
        "If you have a CPU only, you can skip to step \"Examining units that select for trees\" below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCaJHU2XtiBU"
      },
      "source": [
        "from netdissect import segmenter, setting\n",
        "\n",
        "# segmodel = segmenter.UnifiedParsingSegmenter(segsizes=[256])\n",
        "segmodel, seglabels, _ = setting.load_segmenter('netpq')\n",
        "# seglabels = [l for l, c in segmodel.get_label_and_category_names()[0]]\n",
        "print('segmenter has', len(seglabels), 'labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnc4D5sotiBV"
      },
      "source": [
        "Then we create segmentation images for the dataset.  Here tally_cat just concatenates batches of image (or segmentation) data.\n",
        "\n",
        "  * `segmodel.segment_batch` segments an image\n",
        "  * `iv.segmentation(seg)` creates a solid-color visualization of a segmentation\n",
        "  * `iv.segment_key(seg, segmodel)` makes a small legend for the segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "pXMTk3UttiBV"
      },
      "source": [
        "from netdissect import upsample\n",
        "from netdissect import segviz\n",
        "\n",
        "imgs = tally.tally_cat(lambda zbatch: model(zbatch.to(device)), zds)\n",
        "seg = tally.tally_cat(lambda img: segmodel.segment_batch(img.cuda(), downsample=1), imgs)\n",
        "\n",
        "from netdissect.segviz import seg_as_image, segment_key\n",
        "show([\n",
        "    (iv.image(imgs[i]),\n",
        "     iv.segmentation(seg[i,0]),\n",
        "     iv.segment_key(seg[i,0], segmodel)\n",
        "    )\n",
        "    for i in range(min(len(seg), 5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su4jS76UtiBW"
      },
      "source": [
        "## Identifying units that correlate with visual concepts\n",
        "\n",
        "Some units align well with visual concepts.\n",
        "\n",
        "To identify these, we will collect *conditional* activation statistics.\n",
        "\n",
        "In addition to regular quantile statistics, we will collect quantile statistics over all the subsets of pixels in which a particular visual concept is present.\n",
        "\n",
        "To do this, we will use the `tally_conditional_quantile` loop.\n",
        "\n",
        "It expects its `compute` function to return a list of sample statistics, each one keyed by a condition that is present.\n",
        "\n",
        "Here is how we do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQYQQ--QtiBW"
      },
      "source": [
        "# We upsample activations to measure them at each segmentation location.\n",
        "upfn8 = upsample.upsampler((64, 64), (8, 8)) # layer4 is resolution 8x8\n",
        "\n",
        "def compute_conditional_samples(zbatch):\n",
        "    image_batch = model(zbatch.to(device))\n",
        "    seg = segmodel.segment_batch(image_batch, downsample=4)\n",
        "    upsampled_acts = upfn8(model.retained_layer('layer4'))\n",
        "    return tally.conditional_samples(upsampled_acts, seg)\n",
        "\n",
        "# Run this function once to sample one image\n",
        "sample = compute_conditional_samples(zds[0][0].cuda()[None,...])\n",
        "\n",
        "# The result is a list of all the conditional subsamples\n",
        "[(seglabels[c], d.shape) for c, d in sample]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdM-Zc_xtiBW"
      },
      "source": [
        "cq = tally.tally_conditional_quantile(compute_conditional_samples, zds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOcWrm7qtiBW"
      },
      "source": [
        "Conditional quantile statistics let us compute lots of relationships between units and visual concepts.\n",
        "\n",
        "For example, IoU is the \"intersection over union\" ratio, measuring how much overlap there is between the top few percent activations of a unit and the presence of a visual concept.  We can estimate the IoU ratio for all pairs between units and concepts with these stats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpjKD-JYtiBX"
      },
      "source": [
        "iou_table = tally.iou_from_conditional_quantile(cq, cutoff=0.99)\n",
        "iou_table.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whELUT5ptiBX"
      },
      "source": [
        "Now let's view a few of the units, labeled with an associated concept, sorted from highest to lowest IoU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgMpIiE4tiBX"
      },
      "source": [
        "unit_list = sorted(enumerate(zip(*iou_table.max(1))), key=lambda k: -k[1][0])\n",
        "\n",
        "for unit, (iou, segc) in unit_list[:5]:\n",
        "    print('unit %d: %s (iou %.2f)' % (unit, seglabels[segc], iou))\n",
        "    show(unit_viz_row(unit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQUqydQtiBZ"
      },
      "source": [
        "We can quantify the overall match between units and segmentation concepts by counting the number of units that match a segmentation concept (omitting low-scoring matches)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYw2VirGtiBZ"
      },
      "source": [
        "print('Number of units total:', len(unit_list))\n",
        "print('Number of units that match a segmentation concept with IoU > 0.04:',\n",
        "   len([i for i in range(len(unit_list)) if unit_list[i][1][0] > 0.04]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHX3fN1UtiBZ"
      },
      "source": [
        "## Examining units that select for trees\n",
        "\n",
        "Now let's filter just units that were labeled as 'tree' units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_kP7LBLtiBZ"
      },
      "source": [
        "tree_index = seglabels.index('tree')\n",
        "tree_units = [(unit, iou) for iou, unit in\n",
        "              list(zip(*(iou_table[:,tree_index].sort(descending=True))))[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qIXLUYRttiBa"
      },
      "source": [
        "# tree_units = [(unit, iou, segc) for unit, (iou, segc) in unit_list if seglabels[segc] == 'tree'][:10]\n",
        "# If you can't run the segmenter, uncomment the line below and comment the one above.\n",
        "# tree_units = [365, 157, 119, 374, 336, 195, 278, 76, 408, 125]\n",
        "\n",
        "for unit, iou in tree_units:\n",
        "    print('unit %d, iou %.2f' % (unit, iou))\n",
        "    show(unit_viz_row(unit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1fIczqFtiBb"
      },
      "source": [
        "## Editing a model by altering units\n",
        "\n",
        "Now let's try changing some units directly to see what they do.\n",
        "\n",
        "We we will use `model.edit_layer` to do that.\n",
        "\n",
        "This works by just allowing you to define a function that edits the output of a layer.\n",
        "\n",
        "We will edit the output of `layer4` by zeroing ten of the tree units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "00d6TN6YtiBb"
      },
      "source": [
        "def zero_out_tree_units(data, model):\n",
        "    data[:, tree_units, :, :] = 0.0\n",
        "    return data\n",
        "\n",
        "model.edit_layer('layer4', rule=zero_out_tree_units)\n",
        "edited_imgs = tally.tally_cat(lambda zbatch: model(zbatch.to(device)), zds)\n",
        "show([\n",
        "    (['Before', [renormalize.as_image(imgs[i])]],\n",
        "     ['After', [renormalize.as_image(edited_imgs[i])]])\n",
        "      for i in range(min(10, len(zds)))])\n",
        "model.remove_edits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUR7hpDgtiBc"
      },
      "source": [
        "# Exercise 3 - Testing causal effects of representation units\n",
        "\n",
        "Now it's your turn.\n",
        "\n",
        "Now try the following experiments:\n",
        "   * Instead of zeroing the tree units, try setting them negative, e.g., to -5.\n",
        "   * Instead of turning the tree units off, try turning them on, e.g., set them to 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRyD8qY2tiBc"
      },
      "source": [
        "# Your solution to exercise 3.\n",
        "\n",
        "def turn_off_tree_units(data, model):\n",
        "    # your code here\n",
        "    return data\n",
        "\n",
        "model.edit_layer # etc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27VDPigytiBc"
      },
      "source": [
        "# Exercise 4 - Examining units for other concepts\n",
        "\n",
        "\n",
        "Find a set of `door`-selective units `door_units` (instead of `tree_units`), or choose another concepts such as `grass` or `dome`.\n",
        "\n",
        "Then create a set of examples show the effect of setting the these off, to `-5.0` or on, at `10.0`.\n",
        "\n",
        "What is the effect on doors?  What is the effect on other types of objects in the generated scenes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGG4wcCZtiBc"
      },
      "source": [
        "# Your solution to exercise 4.\n",
        "\n",
        "door_index = seglabels.index('door')\n",
        "door_units = [] # find the 10 units with best IoU match for doors\n",
        "\n",
        "def turn_off_door_units(data, model):\n",
        "    data[:, door_units, :, :] = -5.0\n",
        "    return data\n",
        "\n",
        "# Then visualize the effect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2S6sUTltiBd"
      },
      "source": [
        "# Exercise 5 - Comparing layers\n",
        "\n",
        "Collect and visualize the maximum activating images for each unit in `layer5` or `layer6`.\n",
        "\n",
        "Count units that match segmented concepts with IoU > 0.04 in your layer.\n",
        "\n",
        "Are there more matching units than `layer4`, or fewer?\n",
        "\n",
        "Hints:\n",
        "\n",
        "* Skeleton visualization code is below.\n",
        "* To examine the representation at layer5, you will need to `model.retain_layer('layer5')`\n",
        "* layer5 has resolution 16x16 and needs a new upfn function to upsample from that resolution. layer2 is 4x4.\n",
        "* Get conditional quantiles using `tally_conditional_quantile` with a compute_conditional_samples written for layer5.\n",
        "* Then use `iou_from_conditional_quantile` to count matches between units and concepts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIx6hUWtiBd"
      },
      "source": [
        "# Starter code for exercise 5\n",
        "\n",
        "model.retain_layer('layer5')\n",
        "\n",
        "# Visualizations of units at layer5 can be done like this\n",
        "def compute_image_max5(zbatch):\n",
        "    image_batch = model(zbatch.to(device))\n",
        "    return model.retained_layer('layer5').max(3)[0].max(2)[0]\n",
        "\n",
        "# Find the top-activating images for each unit\n",
        "topk5 = tally.tally_topk(compute_image_max5, zds)\n",
        "\n",
        "# Build a visualization for each unit\n",
        "def unit_viz_row5(unitnum, percent_level=0.95):\n",
        "    # Build an array of iv.masked_image for each unitnum,\n",
        "    # Using the top-activating image indexes in topk5\n",
        "    out = []\n",
        "    for imgnum in topk5.result()[1][unitnum][:8]:\n",
        "        # run the model on zds[imgnum]\n",
        "        # collect the retained activations from layer5\n",
        "        # add a masked image (iv.masked_image) to the array\n",
        "        out.append([]) # add your code\n",
        "    return out\n",
        "\n",
        "# Show a few units\n",
        "for unit in range(20):\n",
        "    show(unit_viz_row5(unit))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN0YAT2mtiBd"
      },
      "source": [
        "# Your solution for exercise 5\n",
        "\n",
        "upfn16 = upsample.upsampler((64, 64), (16, 16)) # layer5 is resolution 16x16\n",
        "\n",
        "def compute_layer5_conditional_samples(zbatch):\n",
        "    # Your code here\n",
        "    return tally.conditional_samples(upsampled_acts, seg)\n",
        "\n",
        "cq5 = tally #... your code here\n",
        "iou_table5 = tally #... your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe0nI1nXtiBd"
      },
      "source": [
        "unit_list5 = [] #... find best IoU concept matching each unit\n",
        "print('Number of units matching concepts better than IoU 0.04:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3LkrCgTtiBe"
      },
      "source": [
        "# Extra exercises - Comparing networks\n",
        "\n",
        "Different types of convolutional networks such as image classifiers can be dissected.\n",
        "\n",
        "The main difference is that the segmented concepts are based on input images rather than generated images.\n",
        "\n",
        "To see dissection on a classifier, see the notebook at:\n",
        "[This link](https://colab.research.google.com/github/SIDN-IAP/global-model-repr/blob/master/notebooks/netdissect_exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Su7l4katiBe"
      },
      "source": [
        "One last visualization summarizes dissection - just showing masks and units side-by-side for a few examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WWXiwLcmtiBe"
      },
      "source": [
        "iv2 = imgviz.ImageVisualizer(256, quantiles=rq)\n",
        "model.remove_edits()\n",
        "def compute_acts(zbatch):\n",
        "    model(zbatch.to(device))\n",
        "    return model.retained_layer('layer4')\n",
        "\n",
        "acts = tally.tally_cat(compute_acts, zds).cpu()\n",
        "unit = 365\n",
        "segc = 4\n",
        "\n",
        "if True:\n",
        "    show([\n",
        "        (iv2.image(imgs[i]),\n",
        "         iv2.segmentation(seg[i,0], 4),\n",
        "         iv2.heatmap(acts, (i, unit), mode='nearest'),\n",
        "         iv2.masked_image(imgs[i], acts, (i, unit))\n",
        "        )\n",
        "        for i in [0,2,4,9,25,28]\n",
        "    ])\n",
        "    \n",
        "if False:\n",
        "    show([\n",
        "        (iv2.image(imgs[i]),\n",
        "         iv2.segmentation(seg[i,0], 4),\n",
        "         iv2.heatmap(acts, (i, unit), mode='nearest'),\n",
        "         iv2.masked_image(imgs[i], acts, (i, unit))\n",
        "        )\n",
        "        for i in [1,5,7,8,10,11]\n",
        "    ])\n",
        "\n",
        "if False:\n",
        "    show([\n",
        "        (iv2.image(imgs[i]),\n",
        "         iv2.segmentation(seg[i,0], 11),\n",
        "         iv2.heatmap(acts, (i, 149), mode='nearest'),\n",
        "         iv2.masked_image(imgs[i], acts, (i, 149))\n",
        "        )\n",
        "        for i in [0,2,4,9,25,28]\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExfCkkDKtiBe"
      },
      "source": [
        "Special thanks to Lucy Chai, Ã€gata Lapedriza, and Katherine Gallagher for testing this notebook."
      ]
    }
  ]
}